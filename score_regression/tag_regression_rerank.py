# -*- coding: utf-8 -*-
"""tag_regression_rerank.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vboO9fGvgNLaix3IQ7Irbs2M_sBQP8OG
"""

import numpy as np
import pandas as pd

import scipy.sparse as spr
import pickle
from scipy.sparse import hstack
from collections import Counter

from scipy.io import mmwrite
from scipy.io import mmread

import math
from tqdm import tqdm
import pickle

cd /content/drive/My Drive/Kakao arena

## split.py로 raw train을 train, test로 분리
train = pd.read_json('arena_data/orig/train.json')
test = pd.read_json('arena_data/orig/val.json')
song_meta = pd.read_json('/content/drive/My Drive/Kakao arena/data/meta/song_meta.json', typ = 'frame',encoding='utf-8')
plylst_meta = pd.DataFrame(train[['id','tags','plylst_title']])

"""## 1. tag spare matrix 만들기(target)"""

train['istrain'] = 1
test['istrain'] = 0

n_train = len(train)
n_test = len(test)

# train + test
plylst = pd.concat([train,test], ignore_index=True)
plylst["nid"] = range(n_train+n_test)

plylst_id_nid = dict(zip(plylst["id"],plylst["nid"]))   # nid랑 id 값 각 값이 어떤 값을 나타내는지 저장 dict으로 저장
plylst_nid_id = dict(zip(plylst["nid"],plylst["id"]))   # 앞이 key값, 뒤가 value (id_nid는 id가 key 값, nid가 value 값)

plylst_tag = plylst['tags']
tag_counter = Counter([tg for tgs in plylst_tag for tg in tgs])  # 각 tag가 몇개 있는지 저장 dict을 () 값으로 묶은 Counter 객체
tag_dict = {x: tag_counter[x] for x in tag_counter}             # 그래서 dict으로 풀어줘야함

tag_id_tid = dict()
tag_tid_id = dict()
for i, t in enumerate(tag_dict):            # tag에는 tid 값 부여하기~
    tag_id_tid[t] = i
    tag_tid_id[i] = t

n_tags = len(tag_dict)                  # n_tags에 tag값 부여하기

plylst_song = plylst['songs']           # 각 plylst의 song들
song_counter = Counter([sg for sgs in plylst_song for sg in sgs])
song_dict = {x: song_counter[x] for x in song_counter}

song_id_sid = dict()
song_sid_id = dict()
for i, t in enumerate(song_dict):       # song에는 sid 값 부여하기~
    song_id_sid[t] = i
    song_sid_id[i] = t

n_songs = len(song_dict)

plylst['songs_id'] = plylst['songs'].map(lambda x: [song_id_sid.get(s) for s in x if song_id_sid.get(s) != None]) # get ; key로 value 얻기
plylst['tags_id'] = plylst['tags'].map(lambda x: [tag_id_tid.get(t) for t in x if tag_id_tid.get(t) != None])

## 원래 id 찾아가는 dict
song2id = {v: k for k, v in song_id_sid.items()}
tid2tag = {v:k for k, v in tag_id_tid.items()}

plylst_use = plylst
plylst_use.loc[:,'num_songs'] = plylst_use['songs_id'].map(len)
plylst_use.loc[:,'num_tags'] = plylst_use['tags_id'].map(len)
plylst_use = plylst_use.set_index('nid')

plylst_train = plylst_use.iloc[:n_train,:]
plylst_test = plylst_use.iloc[n_train:,:]

def rating(number):
  return [-math.log(x+1,2) +4.6 for x in range(number)]

row = np.repeat(range(n_train), plylst_train['num_tags'])
col = [tag for tags in plylst_train['tags_id'] for tag in tags]
dat_series = plylst_train['num_tags'].map(rating)
dat = [y for x in dat_series for y in x]
train_tags_A = spr.csr_matrix((dat, (row, col)), shape=(n_train, n_tags))

train_tags_A

tag_target = train_tags_A.T

"""## 2. input X 만들기

### song embedding
"""

X = pd.read_json('/content/drive/My Drive/Kakao arena/X_total.json',orient='table')

X.head(3)

import json
from gensim.models import Word2Vec
from gensim.models import FastText
from gensim.models.doc2vec import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
from gensim.models.keyedvectors import WordEmbeddingsKeyedVectors
import numpy as np
import pandas as pd
from tqdm import tqdm
import gensim  # 3.3.0 not available keyedvectors
# from arena_util import load_json
from sklearn.cluster import KMeans
import sklearn.metrics
import pickle
import re
from collections import Counter

"""
class PlyEmbedding -> 플레이리스트 들어가서 플레이리스트 마다 일정하게 벡터가 부여된다.
class Title2Rec -> 벡터가 부여된 타이틀이 들어가서 cluster, fasttext, T2R
"""

class PlyEmbedding:
    """
    def __init__
     - data: for song based embedding
    
    def make_s2v
     - params: same with Word2Vec params
     
    def make_d2v
     - vector_size: vector size of the Doc2Vec model
     
    def song_based
     - mode: 's2v'(Word2Vec base) or 'd2v'(Doc2Vec base)
     - by:   'mean' or 'sum'
     - keyedvector: True -> return Word2Vec type
                    False-> return (id+titles, vectors) : list
                    플레이리스트 벡터로 근처 플레이리스트 찾을 땐 True
                    클러스터링 학습을 하고 싶을 땐 False
    """
    
    def __init__(self, data):
        super().__init__()
        self.data = data
        self.s2v = None
        self.d2v = None
        print("Data length:", len(data))
        
    def make_s2v(self, min_count=5, size=100, window=5, sg=0):
        songs = [list(map(str, x['songs'])) for x in self.data if len(x['songs']) > 1]
        print("Original data length: ", len(self.data))
        print("Playlist count after remove 0 or 1: ", len(songs))
        print("Traning song2vec...")
        self.s2v = Word2Vec(songs, size=size, window=window,
                            min_count=min_count, sg=sg)
        print("done.")
        
    def get_song2vec(self):
        return self.s2v
        
    def make_d2v(self, vector_size=100):
        doc = [TaggedDocument(list(map(str, x['songs'])),
                        ['(' + str(x['id']) + ') ' + x['plylst_title']]) for x in self.data]
        
        print("Training Doc2Vec...")
        self.d2v = Doc2Vec(doc, vector_size=vector_size, workers=4)
        print('done')
        
    def get_doc2vec(self):
        return self.d2v
    
    def song_based(self, mode='s2v', by='mean', keyedvector=True):
        if mode == 's2v':
            if not self.s2v:
                print("Song2Vec not exist.\nRun make_s2v first.")
                return
        elif mode == 'd2v':
            if not self.d2v:
                print("Doc2Vec not exist.\nRun make_d2v first.")
                return
        else:
            print("mode gets 's2v' or 'd2v'")
            
        if not by in ['mean', 'sum']:
            raise RuntimeError("'by' gets 'mean' or 'sum'")
        
        ply_id = []
        ply_vec = []
        
        for p in tqdm(self.data):
            if by == 'mean':
                tmp = []
            else:
                tmp = 0
            for song in p['songs']:
                try:
                    if by == 'mean':
                        if mode == 's2v':
                            tmp.append(self.s2v.wv.get_vector(str(song)))
                        else:
                            tmp.append(self.d2v.wv.get_vector(str(song)))
                    else:
                        if mode == 's2v':
                            tmp += self.s2v.wv.get_vector(str(song))
                        else:
                            tmp += self.d2v.wv.get_vector(str(song))
                except KeyError:
                    pass
            if by == 'mean':
                if tmp != []:
                    ply_id.append('(' + str(p['id']) + ') ' + p['plylst_title'])
                    ply_vec.append(np.mean(tmp, axis=0))
            else:
                if type(tmp) != int:
                    ply_id.append('(' + str(p['id']) + ') ' + p['plylst_title'])
                    ply_vec.append(tmp)
        
        print("Original data length: ", len(self.data))
        print("Embedded data length: ", len(ply_id))
        
        if not keyedvector:
            return ply_id, ply_vec
        
        out = WordEmbeddingsKeyedVectors(vector_size=100)
        out.add(ply_id, ply_vec)
        
        return out
        
    def by_autoencoder(self):
        pass

""" raw train set으로 song_embedding
    w2v 학습 후 over_5에 적용
"""

train_path = "arena_data/orig/train.json"

train = load_json(train_path)

embed = PlyEmbedding(train)

embed.make_s2v(size=32)

m = embed.get_song2vec()

song_vector = m.wv

song = song_vector.vocab.keys()
song_vector_lst = [song_vector[v] for v in song]

def song_embed(x):
    tem = []
    for s in x:
        try:
            tem.append(song_vector.get_vector(s))
        except KeyError as e:
            pass
    if tem == []:
        return np.zeros(32)
    else:
        return np.mean(tem,axis=0)

song_vector.vocab.keys()

train = pd.read_json('arena_data/orig/train.json')
tem = list(map(str, train['songs']))

X_songembed = pd.concat([X, pd.DataFrame(list(pd.Series(tem).apply(song_embed)))],axis =1 )

X_songembed.rename(columns = {0:'song_0',1:'song_1',2:'song_2',3:'song_3',4:'song_4',5:'song_5',6:'song_6',7:'song_7',8:'song_8',9:'song_9',10:'song_10',11:'song_11',12:'song_12',
                          13:'song_13',14:'song_14',15:'song_15',16:'song_16',17:'song_17',18:'song_18',19:'song_19',20:'song_20',21:'song_21',22:'song_22',23:'song_23',
                          24:'song_24',25:'song_25',26:'song_26',27:'song_27',28:'song_28',29:'song_29',30:'song_30',31:'song_31'},inplace=True)

X_songembed = X_songembed.drop('plylst_title',axis=1)
X_songembed = X_songembed.drop('id',axis=1)
X_songembed = X_songembed.drop('gn_songs',axis=1)

X_songembed.head(3)

X_songembed = X_songembed.drop(X_songembed.columns[list(range(34,50))], axis='columns')

df_season = pd.get_dummies(X_songembed['season']).add_prefix('season') 
df_year = pd.get_dummies(X_songembed['year_section']).add_prefix('year_section') 
X_train = pd.concat([X_songembed,df_season,df_year],axis=1)

del X_train['season']
del X_train['year_section']

X_train.rename(columns={'season1.0':'season_1','season2.0':'season_2','season3.0':'season_3','season4.0':'season_4','year_section1.0':'year_1','year_section2.0':'year_2','year_section3.0':'year_3','year_section4.0':'year_4','year_section5.0':'year_5'})

## cnt 변수 정규화
X_train['artist_id_basket_count'] = (X_train['artist_id_basket_count'] - X_train['artist_id_basket_count'].mean())/X_train['artist_id_basket_count'].std()
X_train['song_gn_gnr_basket_count'] = (X_train['song_gn_gnr_basket_count'] -X_train['song_gn_gnr_basket_count'].mean())/X_train['song_gn_gnr_basket_count'].std()

X_train

## input X 저장
X_train.to_json('tag_train',orient='table')

"""
artist_cnt, gn_cnt, season, year_section -> 11차원
장르 cnt-hot-encoding : 비율 -> 30차원
song w2v -> 32차원

총 input features 73차원
"""

## target 저장
mmwrite('tag_target.mtx',tag_target)

## tid2tag 저장
file=open("tid2tag","wb") 
pickle.dump(tid2tag,file) 
file.close()

## target + input > spc matrix

X_train = X_train.astype(float)
X_spr = spr.csr_matrix(X_train)
X_spr

tag_target

full = hstack((X_spr,tag_target.T))
full = full.tocsc() ; full

mmwrite('tag_full.mtx', full)